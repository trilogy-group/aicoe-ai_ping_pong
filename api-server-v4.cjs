const express = require("express");
const cors = require("cors");
const path = require("path");
const Anthropic = require("@anthropic-ai/sdk");
require("dotenv").config();

const app = express();
// Respect PORT env var (e.g. Render sets it) with a sensible default
const port = process.env.PORT || 3002;

// Middleware
app.use(cors());
app.use(express.json({ limit: "50mb" }));
// Serve static assets generated by `npm run build`
app.use(express.static(path.join(__dirname, "dist")));

// Model driver implementations with search capabilities
const modelHandlers = {
  async gpt(prompt, context, useSearch = false) {
    try {
      if (!process.env.OPENAI_API_KEY) {
        throw new Error("OpenAI API key not configured");
      }

      const OpenAI = require("openai");
      const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

      console.log(
        `ü§ñ GPT request (${prompt.length} chars)${
          useSearch ? " with search" : ""
        }`
      );

      const messages = [
        {
          role: "system",
          content:
            "You are a helpful AI assistant focused on providing practical, actionable responses with accurate, up-to-date information. When conducting research, always include proper source citations in format [Source: Publication Name, Article Title, Date, URL]. Stay focused on the user's specific request and avoid generic business language. Be engaging and creative while remaining helpful and direct.",
        },
        { role: "user", content: context.userInput },
        ...context.stepOutputs
          .slice(-2)
          ?.map((c) => ({ role: "user", content: c })),
        {
          role: "user",
          content: prompt,
        },
      ];

      const requestConfig = {
        model: "gpt-4.1",
        tools: [{ type: "web_search_preview" }],
        input:
          prompt +
          "\nBased on the following context:\n" +
          messages.map((m) => `${m.role}: ${m.content}`).join("\n"),
      };

      const res = await openai.responses.create(requestConfig);

      const content = res.output[res.output.length - 1].content;
      const text = content.text || res.output_text;
      const citations =
        content.annotations?.map(
          (a) =>
            `[${a.type}] ${a.title} - ${a.url} (${a.start_index} - ${a.end_index})`
        ) || [];

      return `${text}\n\n${citations?.join("\n")}`;
    } catch (error) {
      console.error("GPT API error:", error);
      return "GPT API error: " + error.message;
    }
  },

  async claude(prompt, context, useSearch = false) {
    try {
      if (!process.env.ANTHROPIC_API_KEY) {
        throw new Error("Anthropic API key not configured");
      }

      const anthropic = new Anthropic({
        apiKey: process.env.ANTHROPIC_API_KEY,
      });

      console.log(
        `üß† Claude request (${prompt.length} chars)${
          useSearch ? " with search and thinking" : ""
        }`
      );

      // Determine system prompt
      const isFinalArticleStep =
        prompt.includes("ultimate-final-article") ||
        prompt.includes(
          "DELIVER ONLY the complete, final, publication-ready article content"
        ) ||
        prompt.includes("Begin the article immediately");

      const defaultSystemPrompt =
        "You are Claude, an AI assistant focused on logical analysis, accuracy, and clear reasoning. " +
        "When conducting research, always include proper source citations in format [Source: Publication Name, Article Title, Date, URL]. " +
        "When tackling complex requests, think step by step and show your reasoning as analysis, followed by a clear answer.";

      const finalArticlePrompt =
        "You are Claude, a master content creator. For final article production: DO NOT provide thinking, analysis, or meta-commentary. " +
        "DELIVER ONLY the complete, final article content. Begin immediately with the article title and content.";

      const systemPrompt = isFinalArticleStep
        ? finalArticlePrompt
        : defaultSystemPrompt;

      const requestConfig = {
        model: "claude-opus-4-20250514",
        max_tokens: 32000,
        system: systemPrompt,
        messages: [
          { role: "user", content: context.userInput },
          ...context.stepOutputs
            .slice(-2)
            .map((c) => ({ role: "user", content: c })),
          { role: "user", content: prompt },
        ],
        tools: [
          {
            name: "web_search",
            type: "web_search_20250305",
          },
        ],
        thinking: {
          type: "enabled",
          budget_tokens: 31999,
        },
      };

      // Send request as a streaming response
      const response = await anthropic.messages.create(requestConfig);

      console.log("üß† Claude response:", response);
      const msg = response.content
        .filter((c) => c.type === "text")
        .map((c) => c.text)
        .join("\n");

      return msg;
    } catch (error) {
      console.error("Claude API error:", error);
      return "Claude API error: " + error.message;
    }
  },

  async gemini(prompt, context, useSearch = false) {
    // -- NEW implementation using @google/genai official SDK with streaming --
    try {
      if (!process.env.GEMINI_API_KEY) {
        throw new Error("Gemini API key not configured");
      }

      console.log(
        `üíé Gemini request (${prompt.length} chars)$${
          useSearch ? " with search" : ""
        }`
      );

      // Lazy-load the SDK to avoid cost if Gemini is disabled
      const { GoogleGenAI } = require("@google/genai");

      const ai = new GoogleGenAI({
        apiKey: process.env.GEMINI_API_KEY,
      });

      const modelName = "gemini-2.5-pro";

      // Build config similar to user snippet
      const config = {
        temperature: 1.05,
        thinkingConfig: {
          thinkingBudget: -1,
        },
        responseMimeType: "text/plain",
      };

      // Combine context + prompt
      const combinedText = [
        context.userInput || "",
        ...context.stepOutputs.slice(-3),
        prompt,
      ]
        .filter(Boolean)
        .join("\n\n");

      const baseMessage = {
        role: "user",
        parts: [
          {
            text: combinedText,
          },
        ],
      };

      // Attach Google search retrieval tool ONLY if search requested and model is 1.5
      const requestParams = {
        model: modelName,
        config,
        contents: [baseMessage],
      };

      if (useSearch && modelName.startsWith("gemini-1.5")) {
        requestParams.tools = [
          {
            google_search_retrieval: {
              dynamic_retrieval_config: {
                mode: "MODE_DYNAMIC",
                dynamic_threshold: 0.7,
              },
            },
          },
        ];
      }

      // --- Retry logic similar to previous implementation ---
      const { maxRetries, retryDelay, backoffMultiplier } =
        modelsConfig.retryPolicy;

      let attempt = 0;
      let delayMs = retryDelay;
      const sleep = (ms) => new Promise((res) => setTimeout(res, ms));

      let lastError;

      while (attempt <= maxRetries) {
        try {
          const stream = await ai.models.generateContentStream(requestParams);

          let fullText = "";
          for await (const chunk of stream) {
            if (chunk.text) fullText += chunk.text;
          }

          if (!fullText.trim()) {
            throw new Error("No content returned from Gemini stream");
          }

          return fullText;
        } catch (err) {
          lastError = err;

          // Retry only on transient errors
          if (
            attempt < maxRetries &&
            ["ECONNRESET", "ETIMEDOUT"].includes(err.code) // network level
          ) {
            console.warn(
              `‚ö†Ô∏è Gemini attempt #${attempt + 1} failed: ${
                err.message
              }. Retrying in ${delayMs} ms‚Ä¶`
            );
            await sleep(delayMs);
            delayMs *= backoffMultiplier;
            attempt += 1;
            continue;
          }

          break; // non-retryable
        }
      }

      console.error("‚ùå Gemini failed:", lastError?.message || lastError);

      return "Gemini API error: " + (lastError?.message || "Unknown error");
    } catch (error) {
      console.error("Gemini API setup error:", error);
      return "Gemini API error: " + error.message;
    }
  },

  async grok(prompt, context, useSearch = false) {
    try {
      if (!process.env.GROK_API_KEY) {
        // Fallback to Gemini if available
        if (process.env.GEMINI_API_KEY) {
          console.log(
            "üîÑ Grok unavailable, falling back to Gemini with search"
          );
          return await modelHandlers.gemini(
            `Acting as Grok with real-time research capabilities: ${prompt}`,
            context,
            useSearch
          );
        }
        throw new Error(
          "Grok API key not configured and no fallback available"
        );
      }

      const axios = require("axios");
      console.log(
        `‚ö° Grok request (${prompt.length} chars)${
          useSearch ? " with deep search" : ""
        }`
      );

      const requestConfig = {
        model: "grok-3",
        system:
          "You are Grok, an AI with real-time information access and a witty personality. When conducting research, always include proper source citations in format [Source: Publication Name, Article Title, Date, URL]. Focus on STEM/math reasoning, live social-web research, and chain-of-thought analysis. Provide current, accurate information while staying focused on the user's specific request.",
        messages: [
          {
            role: "user",
            content: context.userInput,
          },
          ...context.stepOutputs
            .slice(-3)
            .map((c) => ({ role: "user", content: c })),
          {
            role: "user",
            content: prompt,
          },
          {
            role: "user",
            content: prompt,
          },
        ],
        temperature: 1,
        stream: false,
      };

      // Note: Grok may have native search without explicit tool configuration
      // The search capability is inherent to the model itself

      const response = await axios.post(
        "https://api.x.ai/v1/chat/completions",
        requestConfig,
        {
          headers: {
            Authorization: `Bearer ${process.env.GROK_API_KEY}`,
            "Content-Type": "application/json",
          },
        }
      );

      return response.data.choices[0].message.content;
    } catch (error) {
      console.error("Grok API error:", error);
      return "Grok API error: " + error.message;
    }
  },
};

// Workflow configuration
const modelsConfig = {
  stepOverrides: {
    article: {
      "clarify-brief": "gpt",
      "deep-research": "gemini",
      "first-draft": "gpt",
      "strengths-analysis": "grok",
      "initial-structure": "gpt",
      "logical-enhancement": "claude",
      "creative-polish": "gpt",
      "final-logic-check": "claude",
      "production-ready": "gpt",
    },
    email: {
      "decode-request": "gpt",
      "validate-response": "gemini",
      "craft-response": "gpt",
    },
    research: {
      "clarify-scope": "gpt",
      "broad-research": "gemini",
      "structure-findings": "gpt",
      "deep-dive-research": "gemini",
      "logical-analysis": "claude",
      "strategic-synthesis": "gpt",
      "final-report": "claude",
    },
  },
  fallbacks: {
    grok: "gemini",
    gemini: "gpt",
    claude: "gpt",
    gpt: null,
  },
  enabledDrivers: {
    gpt: true,
    claude: true,
    gemini: true,
    grok: true, // Now enabled by default since it's working
  },
  retryPolicy: {
    maxRetries: 3,
    retryDelay: 1000,
    backoffMultiplier: 2,
  },
};

// Workflow Engine
class WorkflowEngine {
  resolveModel(stepModel, stepId, scenarioId) {
    // Check step-specific overrides first
    const stepOverrides = modelsConfig.stepOverrides[scenarioId];
    let targetModel = stepOverrides?.[stepId] || stepModel;

    // Handle special "all-models" validation step
    if (targetModel === "all-models") {
      return "all-models";
    }

    // Apply fallback logic if the target model is disabled
    while (targetModel && !modelsConfig.enabledDrivers[targetModel]) {
      const fallback = modelsConfig.fallbacks[targetModel];
      if (fallback === null) {
        return null; // No more fallbacks available
      }
      targetModel = fallback;
      console.log(
        `üîÑ Model ${stepModel} disabled, falling back to ${targetModel}`
      );
    }

    return targetModel;
  }

  renderPrompt(template, context) {
    let rendered = template;

    // Replace {{userInput}} placeholder
    if (context.userInput) {
      rendered = rendered.replace(/\{\{userInput\}\}/g, context.userInput);
    }

    // Replace {{stepN}} placeholders with previous outputs
    context.stepOutputs?.forEach((output, index) => {
      if (output && output.trim()) {
        const placeholder = `{{step${index + 1}}}`;
        rendered = rendered.replace(
          new RegExp(placeholder.replace(/[{}]/g, "\\$&"), "g"),
          output
        );
      }
    });

    // Handle validation-specific placeholders
    if (context.validationResults) {
      rendered = rendered.replace(
        /\{\{gptValidation\}\}/g,
        context.validationResults.gpt || "No assessment"
      );
      rendered = rendered.replace(
        /\{\{claudeValidation\}\}/g,
        context.validationResults.claude || "No assessment"
      );
      rendered = rendered.replace(
        /\{\{geminiValidation\}\}/g,
        context.validationResults.gemini || "No assessment"
      );
      rendered = rendered.replace(
        /\{\{grokValidation\}\}/g,
        context.validationResults.grok || "No assessment"
      );
      rendered = rendered.replace(
        /\{\{originalContent\}\}/g,
        context.originalContent || "No content"
      );
    }

    // Replace {{content}} with the latest output for validation steps
    if (context.stepOutputs && context.stepOutputs.length > 0) {
      const latestOutput = context.stepOutputs[context.stepOutputs.length - 1];
      rendered = rendered.replace(/\{\{content\}\}/g, latestOutput || "");
    }

    return rendered;
  }

  // Determine if a step should use search capabilities
  shouldUseSearch(stepId, prompt) {
    // Steps that definitely need search
    const searchRequiredSteps = [
      "deep-research",
      "validate-response",
      "broad-research",
      "deep-dive-research",
      "multi-model-source-validation",
      "final-source-verification",
    ];

    // Keywords that indicate search should be used
    const searchKeywords = [
      "current",
      "latest",
      "recent",
      "up-to-date",
      "trending",
      "statistics",
      "data",
      "research",
      "sources",
      "verify",
      "validate",
      "fact-check",
      "confirm",
      "investigate",
      "WITH SOURCES",
      "CRITICAL:",
      "citations",
      "references",
    ];

    // Check if step requires search
    if (searchRequiredSteps.includes(stepId)) {
      return true;
    }

    // Check if prompt contains search-indicating keywords
    const lowerPrompt = prompt.toLowerCase();
    return searchKeywords.some((keyword) =>
      lowerPrompt.includes(keyword.toLowerCase())
    );
  }

  async executeMultiModelValidation(prompt, context) {
    console.log(`üîç Starting multi-model source validation with search`);

    const validationConfig = {
      gpt: "As GPT with web search capabilities, perform rigorous fact-checking on the sources and claims in the following content. Use search to verify facts and cross-check sources. Focus on logical consistency, factual accuracy, and identifying any potential misinformation. Include source citations for your verification:\n\n{{content}}",
      claude:
        "As Claude with thinking and web search, analyze the following content for source reliability and logical reasoning. Use search to verify claims and sources. Identify any potential hallucinations, unsupported claims, or questionable sources. Provide detailed reasoning for each assessment with citations:\n\n{{content}}",
      gemini:
        "As Gemini with Google grounding search, verify the sources and factual claims in the following content. Use grounding search to cross-reference with current information and identify any inconsistencies or unreliable sources. Include source citations:\n\n{{content}}",
      grok: "As Grok with real-time web access and deep search, validate the sources and current information in the following content. Use your native search capabilities to check for accuracy, recency, and reliability of claims, especially those related to recent events or data. Include source citations:\n\n{{content}}",
    };

    const models = ["gpt", "claude", "gemini", "grok"];
    const validationResults = {};

    // Run validation in parallel across all enabled models WITH SEARCH
    const validationPromises = models
      .filter((model) => modelsConfig.enabledDrivers[model])
      .map(async (model) => {
        try {
          console.log(
            `üîç Running ${model.toUpperCase()} validation with search`
          );
          const validationPrompt = this.renderPrompt(
            validationConfig[model],
            context
          );
          // Use search for all validation models
          const result = await modelHandlers[model](
            validationPrompt,
            context,
            true
          );

          validationResults[model] = result;
          console.log(
            `‚úÖ ${model.toUpperCase()} validation complete with search (${
              result.length
            } chars)`
          );
        } catch (error) {
          console.error(
            `‚ùå ${model.toUpperCase()} validation failed:`,
            error.message
          );
          validationResults[model] = `Validation failed: ${error.message}`;
        }
      });

    await Promise.all(validationPromises);

    // Store validation results in context for next step
    context.validationResults = validationResults;
    context.originalContent =
      context.stepOutputs[context.stepOutputs.length - 1];

    // Return validation summary
    const summaryText =
      `Multi-model source validation with search completed:\n\n` +
      Object.entries(validationResults)
        .map(
          ([model, result]) =>
            `${model.toUpperCase()} Assessment with Search (${
              result.length
            } chars):\n${result.substring(0, 200)}...`
        )
        .join("\n\n");

    return summaryText;
  }

  async executeStep(step, context, scenarioId) {
    // Validate step object
    if (!step || typeof step !== "object") {
      throw new Error("Invalid step object provided");
    }

    if (!step.id) {
      throw new Error("Step object missing required 'id' property");
    }

    if (!step.model) {
      throw new Error("Step object missing required 'model' property");
    }

    // ENHANCED: Check if this is an enhanced workflow step with capabilities
    if (step.capabilities && Array.isArray(step.capabilities)) {
      return await this.executeEnhancedStep(step, context, scenarioId);
    }

    // Legacy workflow execution path
    const targetModel = this.resolveModel(step.model, step.id, scenarioId);

    if (!targetModel) {
      throw new Error("No available model for this step");
    }

    // Handle special multi-model validation step
    if (targetModel === "all-models") {
      return await this.executeMultiModelValidation(
        step.rawPrompt || step.prompt,
        context
      );
    }

    // Handle final verification step with validation context
    if (step.id === "final-source-verification" && context.validationResults) {
      const verificationPrompt = `You are Claude with thinking and web search capabilities. You have received source validation assessments from all 4 AI models (GPT, Claude, Gemini, and Grok). Your task is to:

1. Use web search to independently verify any questionable sources identified
2. Analyze all validation reports with special trust for Grok and Gemini's source assessments  
3. Identify any hallucinated or unreliable sources that need removal
4. Provide a final cleaned version of the content with problematic sources removed
5. List what changes were made and why, with search-verified reasoning

Validation Reports:
GPT Assessment: {{gptValidation}}
Claude Assessment: {{claudeValidation}}
Gemini Assessment: {{geminiValidation}}
Grok Assessment: {{grokValidation}}

Original Content:
{{originalContent}}

Use your search capabilities to verify any disputed sources and provide your final verified content with change summary.`;

      const renderedPrompt = this.renderPrompt(verificationPrompt, context);
      console.log(
        `üöÄ Workflow Engine: Final source verification with Claude + search`
      );

      return await modelHandlers.claude(renderedPrompt, context, true);
    }

    const renderedPrompt = this.renderPrompt(
      step.rawPrompt || step.prompt,
      context
    );

    // Determine if this step should use search
    const useSearch = this.shouldUseSearch(step.id, renderedPrompt);

    console.log(
      `üöÄ Workflow Engine: Executing step "${
        step.id
      }" with ${targetModel.toUpperCase()}${useSearch ? " + search" : ""}`
    );

    return await modelHandlers[targetModel](renderedPrompt, context, useSearch);
  }

  // NEW: Enhanced workflow step execution with capability-based routing
  async executeEnhancedStep(step, context, scenarioId) {
    console.log(
      `üéØ Enhanced Workflow: Processing step "${
        step.id
      }" with capabilities: [${step.capabilities.join(", ")}]`
    );

    // Capability-based model selection
    const targetModel = this.selectOptimalModel(
      step.capabilities,
      step.prioritizeBy
    );

    if (!targetModel) {
      throw new Error(
        `No model available for capabilities: ${step.capabilities.join(", ")}`
      );
    }

    const routingReason = `Selected for capabilities: ${step.capabilities.join(
      "+"
    )} (optimized for ${step.prioritizeBy || "accuracy"})`;
    console.log(
      `üéØ Capability routing: ${step.capabilities.join(
        "+"
      )} ‚Üí ${targetModel.toUpperCase()} (${routingReason})`
    );

    const renderedPrompt = this.renderPrompt(
      step.rawPrompt || step.prompt,
      context
    );

    // Enhanced steps that need search capabilities
    const useSearch =
      this.shouldUseSearch(step.id, renderedPrompt) ||
      step.capabilities.includes("live-web") ||
      step.capabilities.includes("real-time-data") ||
      step.capabilities.includes("fact-checking");

    console.log(
      `üöÄ Enhanced Workflow Engine: Executing step "${
        step.id
      }" with ${targetModel.toUpperCase()}${useSearch ? " + search" : ""}`
    );

    const result = await modelHandlers[targetModel](
      renderedPrompt,
      context,
      useSearch
    );

    // Enhanced validation if requested
    if (step.requiresValidation && step.validationType) {
      console.log(`üîç Running enhanced ${step.validationType} validation`);
      return await this.runEnhancedValidation(
        result,
        step.validationType,
        context
      );
    }

    return result;
  }

  // NEW: Capability-based model selection (simplified version for API server)
  selectOptimalModel(capabilities, prioritizeBy = "accuracy") {
    // Model capability mappings (updated to match enhanced workflow capabilities)
    const modelCapabilities = {
      gpt: [
        "creative-polish",
        "json-output",
        "human-like-voice",
        "creative_polish",
        "json_output",
        "fact_checking",
        "structural-analysis",
      ],
      claude: [
        "long-form-narrative",
        "structural-analysis",
        "logical-reasoning",
        "comprehensive-review",
        "logic-audit",
        "fact-checking",
        "source_verification",
        "logic_audit",
        "long_form_narrative",
      ],
      gemini: [
        "high-context-analysis",
        "document-analysis",
        "comprehensive-review",
        "fact-checking",
        "source-verification",
        "high_volume_research",
        "multimodal_rag",
        "document_analysis",
        "long_context",
      ],
      grok: [
        "live-web",
        "real-time-data",
        "fact-checking",
        "high-volume-research",
        "stem_math_reasoning",
        "real_time_data",
        "live_web",
        "source_validation",
      ],
    };

    // Find models that can handle ALL required capabilities
    const availableModels = Object.keys(modelCapabilities).filter(
      (model) =>
        modelsConfig.enabledDrivers[model] &&
        capabilities.every((cap) => modelCapabilities[model].includes(cap))
    );

    if (availableModels.length === 0) {
      // Fallback: find model with most matching capabilities
      const candidateModels = Object.keys(modelCapabilities)
        .filter((model) => modelsConfig.enabledDrivers[model])
        .map((model) => ({
          model,
          matchScore: capabilities.filter((cap) =>
            modelCapabilities[model].includes(cap)
          ).length,
        }))
        .sort((a, b) => b.matchScore - a.matchScore);

      if (candidateModels.length > 0) {
        console.log(
          `‚ö†Ô∏è No perfect match for [${capabilities.join(
            ", "
          )}], using best available: ${candidateModels[0].model.toUpperCase()}`
        );
        return candidateModels[0].model;
      }
      return null;
    }

    // Priority-based selection among capable models
    if (prioritizeBy === "context" && availableModels.includes("gemini")) {
      return "gemini"; // 1M context window
    }
    if (prioritizeBy === "accuracy" && availableModels.includes("claude")) {
      return "claude"; // Highest accuracy score
    }
    if (prioritizeBy === "cost" && availableModels.includes("gemini")) {
      return "gemini"; // Lowest cost tier
    }
    if (prioritizeBy === "latency" && availableModels.includes("gpt")) {
      return "gpt"; // Fastest response
    }

    // Default to first available model
    return availableModels[0];
  }

  // NEW: Enhanced validation using cross-model checking
  async runEnhancedValidation(content, validationType, context) {
    console.log(`üîç Running enhanced ${validationType} validation`);

    const validationPrompts = {
      fact_check: `You are a fact-checking expert. Analyze the following content for factual accuracy and logical consistency. Identify any questionable claims, unsupported statements, or potential misinformation. Rate your confidence in the content's accuracy (1-10):\n\n${content}`,
      source_verify: `You are a source verification specialist. Examine the following content for source reliability, citation quality, and reference authenticity. Identify any questionable sources or unsupported claims. Rate source reliability (1-10):\n\n${content}`,
      logic_audit: `You are a logic and reasoning expert. Analyze the following content for logical consistency, argument structure, and reasoning quality. Identify any logical fallacies, inconsistencies, or weak arguments. Rate logical coherence (1-10):\n\n${content}`,
    };

    // Use two different models for cross-validation
    const validationModels =
      validationType === "fact_check"
        ? ["gpt", "claude"]
        : validationType === "source_verify"
        ? ["grok", "gemini"]
        : ["claude", "gpt"]; // logic_audit

    const prompt = validationPrompts[validationType];

    try {
      const validationPromises = validationModels
        .filter((model) => modelsConfig.enabledDrivers[model])
        .slice(0, 2) // Limit to 2 models for performance
        .map(async (model) => {
          try {
            return await modelHandlers[model](prompt, context, true); // Use search for validation
          } catch (error) {
            return `Validation failed: ${error.message}`;
          }
        });

      const validationResults = await Promise.all(validationPromises);

      // Return original content with validation summary
      return `${content}\n\n---\nüîç Enhanced ${validationType} validation completed:\n${validationResults
        .map(
          (result, i) =>
            `${validationModels[i].toUpperCase()}: ${result.substring(
              0,
              200
            )}...`
        )
        .join("\n\n")}`;
    } catch (error) {
      console.error(`‚ùå Enhanced validation failed:`, error.message);
      return content; // Return original content if validation fails
    }
  }

  toggleDriver(driverName, enabled) {
    modelsConfig.enabledDrivers[driverName] = enabled;
    console.log(`üîß Driver ${driverName} ${enabled ? "enabled" : "disabled"}`);
  }

  getDriverStatus() {
    return { ...modelsConfig.enabledDrivers };
  }
}

const workflowEngine = new WorkflowEngine();

// API Routes
app.post("/api/run", async (req, res) => {
  try {
    const { step, context, scenarioId } = req.body;

    // Validate required parameters
    if (!step) {
      return res.status(400).json({
        success: false,
        error: "Missing required parameter: step",
      });
    }

    if (!step.id) {
      return res.status(400).json({
        success: false,
        error: "Missing required parameter: step.id",
      });
    }

    if (!step.model) {
      return res.status(400).json({
        success: false,
        error: "Missing required parameter: step.model",
      });
    }

    if (!context) {
      return res.status(400).json({
        success: false,
        error: "Missing required parameter: context",
      });
    }

    console.log(`\nüöÄ Processing request for step "${step.id}"`);

    const targetModel = workflowEngine.resolveModel(
      step.model,
      step.id,
      scenarioId || "article"
    );
    if (!targetModel) {
      return res.status(400).json({
        success: false,
        error: "No available model for this step",
      });
    }

    const result = await workflowEngine.executeStep(
      step,
      context,
      scenarioId || "article"
    );

    console.log(result);

    res.json({
      success: true,
      result,
    });
  } catch (error) {
    console.error("‚ùå API Error:", error.message);
    console.error("‚ùå Stack trace:", error.stack);

    if (!res.headersSent) {
      res.status(500).json({
        success: false,
        error: error.message,
      });
    }
  }
});

// Legacy streaming API deprecation
app.post("/api/stream", async (req, res) => {
  return res.status(410).json({
    success: false,
    error: "Streaming API deprecated. Use /api/run for regular responses.",
    migration:
      "Update client to use regular HTTP requests with /api/run endpoint",
  });
});

// Driver management
app.post("/api/drivers/toggle", (req, res) => {
  try {
    const { driver, enabled } = req.body;
    workflowEngine.toggleDriver(driver, enabled);

    res.json({
      success: true,
      drivers: workflowEngine.getDriverStatus(),
    });
  } catch (error) {
    res.status(500).json({
      success: false,
      error: error.message,
    });
  }
});

app.get("/api/drivers", (req, res) => {
  res.json({
    success: true,
    drivers: workflowEngine.getDriverStatus(),
    capabilities: {
      gpt: {
        strengths: [
          "logic_audit",
          "creative_polish",
          "fact_checking",
          "json_output",
        ],
        contextLimit: 128000,
        costTier: "medium",
      },
      claude: {
        strengths: [
          "long_form_narrative",
          "structural_analysis",
          "logical_reasoning",
        ],
        contextLimit: 200000,
        costTier: "medium",
      },
      gemini: {
        strengths: [
          "high_volume_research",
          "multimodal_rag",
          "document_analysis",
        ],
        contextLimit: 1000000,
        costTier: "low",
      },
      grok: {
        strengths: [
          "stem_math_reasoning",
          "real_time_data",
          "social_web_search",
        ],
        contextLimit: 128000,
        costTier: "high",
      },
    },
  });
});

// Health check
app.get("/api/health", (req, res) => {
  res.json({
    status: "ok",
    timestamp: new Date().toISOString(),
    version: "4.0.0-standard",
    models: {
      gpt: !!process.env.OPENAI_API_KEY,
      claude: !!process.env.ANTHROPIC_API_KEY,
      gemini: !!process.env.GEMINI_API_KEY,
      grok: !!process.env.GROK_API_KEY,
    },
    drivers: workflowEngine.getDriverStatus(),
  });
});

// ---- SPA fallback ----
// Serve index.html for any non-API request (client-side routing)
app.use((req, res, next) => {
  // Pass through for API routes or static asset requests that actually exist
  if (req.path.startsWith("/api") || req.path.includes(".")) {
    return next();
  }
  res.sendFile(path.join(__dirname, "dist", "index.html"));
});

app.listen(port, () => {
  console.log(
    `üöÄ AI Ping-Pong Studio v4 API Server running on http://localhost:${port}`
  );
  console.log(`üîó Main endpoint: http://localhost:${port}/api/run`);
  console.log(`\nüîë API Key Status:`);
  console.log(
    `   OpenAI (GPT): ${
      process.env.OPENAI_API_KEY ? "‚úÖ Configured" : "‚ùå Missing"
    }`
  );
  console.log(
    `   Anthropic (Claude): ${
      process.env.ANTHROPIC_API_KEY ? "‚úÖ Configured" : "‚ùå Missing"
    }`
  );
  console.log(
    `   Google (Gemini): ${
      process.env.GEMINI_API_KEY ? "‚úÖ Configured" : "‚ùå Missing"
    }`
  );
  console.log(
    `   xAI (Grok): ${
      process.env.GROK_API_KEY ? "‚úÖ Configured" : "‚ùå Missing"
    }`
  );

  console.log(`\nüîß Driver Status:`);
  const drivers = workflowEngine.getDriverStatus();
  Object.entries(drivers).forEach(([name, enabled]) => {
    console.log(
      `   ${name.toUpperCase()}: ${enabled ? "‚úÖ Enabled" : "‚ùå Disabled"}`
    );
  });

  if (
    !process.env.OPENAI_API_KEY &&
    !process.env.ANTHROPIC_API_KEY &&
    !process.env.GEMINI_API_KEY
  ) {
    console.log(`\n‚ùó WARNING: No API keys found!`);
    console.log(`   Add API keys to your .env file:`);
    console.log(`   OPENAI_API_KEY=your_openai_key`);
    console.log(`   ANTHROPIC_API_KEY=your_anthropic_key`);
    console.log(`   GEMINI_API_KEY=your_gemini_key`);
    console.log(`   GROK_API_KEY=your_grok_key`);
  }
  console.log(``);
});
